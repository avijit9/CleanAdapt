<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>CleanAdapt</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:30px">Overcoming Label Noise for Source-free Unsupervised Video Domain Adaptation</span>
		<br>
		<br>
		<br>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://avijit9.github.io/">Avijit Dasgupta</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?hl=en&user=U9dH-DoAAAAJ&view_op=list_works&sortby=pubdate">C. V. Jawahar</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://thoth.inrialpes.fr/people/alahari/">Karteek Alahari</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<br>
			<table align=center width=600px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href='https://dl.acm.org/doi/abs/10.1145/3571600.3571621'>[Conference Paper]</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href='https://www.sciencedirect.com/science/article/pii/S0031320324010793'>[Journal Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/avijit9/CleanAdapt'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<br>
	<br>
	<center>
		<table align=center width=720px>
			<tr>
				<td width=1200px colspan="3">
				  <center>
					  <img src="./resources/motivation.png" width="720px"></img>
				</center>
				</td>
			</tr>
			<tr>
				<td width=600px colspan="3">
					  <span style="font-size:14px">Plot of average cross-entropy loss per epoch of training with pseudo-labeled target domain videos for clean vs. noisy
						samples with (a) RGB modality, and (b) Flow modality. We term the target domain samples with correct pseudo-labels as clean
						samples and with incorrect pseudo-labels as noisy samples. Note that, the groundtruth labels are only used to identify the
						clean vs. the noisy samples for visualization purpose and not used for training the model. Deep neural networks learn the
						clean samples first before memorizing the noisy samples according to the deep memorization effect. In our
						proposed approach CleanAdapt, we exploit this connection to select the clean samples for fine-tuning the model to adapt to the
						target domain. 
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Despite the progress seen in classification methods, current approaches for handling videos with distribution-shift in source and
				target domains remain source-dependent as they require access
				to the source data during the adaptation stage. In this paper, we
				present a self-training based source-free video domain adaptation
				approach (without bells and whistles) to address this challenge by
				bridging the gap between the source and the target domains. We
				use the source pre-trained model to generate pseudo-labels for the
				target domain samples, which are inevitably noisy. We treat the
				problem of source-free video domain adaptation as learning from
				noisy labels and argue that the samples with correct pseudo-labels
				can help us in adaptation. To this end, we leverage the cross-entropy
				loss as an indicator of the correctness of the pseudo-labels, and use
				the resulting small-loss samples from the target domain for fine-tuning the model. Extensive experimental evaluations show that
				our method, termed as CleanAdapt, achieves âˆ¼ 7% gain over the
				source-only model and outperforms the state-of-the-art approaches
				on various open datasets.
			</td>
		</tr>
		
	</table>
	<br>

	<hr>

	<center><h1>Methodology</h1></center>
	
	<table align=center width=720px>
		<tr>
			  <td width=600px colspan=2><center><b>Overview of our CleanAdapt framework</b></center><br></td> 
			  <td width=600px colspan=1><center><b>Clean sample selection module</b></center><br></td> 
		</tr>
		
		<br>
		<tr>
			<td width=700px colspan="2">
			  <center>
				  <img src="./resources/overall_pipeline.png" width="600px"></img>
			</center>
			</td>
			
			<td width=700px colspan="2">
				<center>
					<img src="./resources/SLT.png" width="420px"></img>
			  </center>
			  </td>
		</tr>
	</table>
	<br>
	<br>
	<table align=center width=720px>
		
		<br>
		<tr>
			
		</tr>
	</table>
	<hr>
	<center><h1>Results and Analysis</h1></center>
	<table align=center width=720px>
		<tr>
			<td><center><b></b></center><br></td>
		</tr>
		
		<br>
		<tr>
			<td width=700px colspan="2">
			  <center>
				  <img src="./resources/table1.png" width="300px"></img>
			</center>
			</td>

			<td width=700px colspan="2">
				<center>
					<img src="./resources/tabl2.png" width="600px"></img>
			  </center>
			  </td>
		</tr>
	</table>

	<table align=center width=720px>
		<tr>
			<td><center><b></b></center><br></td>
		</tr>
		
		<br>
		<tr><td><b><center>Retrieval Results on UCF - HMDB Dataset</center></b><br></td></tr>
		<tr>
			
			<td width=700px colspan="2">
			  <center>
				  <img src="./resources/retrieval.png" width="720px"></img>
			</center>
			</td>
		</tr>

	</table>

	

	<hr>
	<table align=center width=800>
		<center><h1>Paper</h1></center>
		   <tr>
			 <td><a href=""><img style="width:200px" src="./resources/paper1.png"></a></td>
			 <td><span style="font-size:11pt">
				Avijit Dasgupta, C. V. Jawahar, Karteek Alahari<br>
				<b>Overcoming Label Noise for Source-free Unsupervised Video Domain Adaptation</b><br>
				ICVGIP 2022 <font color="red">(Best paper runner-up award)</font> <br> <br>
				<a href="https://dl.acm.org/doi/abs/10.1145/3571600.3571621">conference paper</a> &nbsp; / &nbsp;
				 <a href="https://github.com/avijit9/CleanAdapt">code</a>
			 </td>
		</tr>
		<tr>
			<td><a href=""><img style="width:200px" src="./resources/paper2.png"></a></td>
			 <td><span style="font-size:11pt">
				Avijit Dasgupta, C. V. Jawahar, Karteek Alahari<br>
				<b>Source-free video domain adaptation by learning from noisy labels</b><br>
				Pattern Recognition 2025 <font color="red">(I.F. - 7.5)</font>  <br> <br>
				<a href="https://www.sciencedirect.com/science/article/pii/S0031320324010793">journal paper</a> &nbsp; / &nbsp;
				 <a href="https://github.com/avijit9/CleanAdapt">code</a>
			 </td>
		 </tr>
	   </table>
	<br>
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>

					Avijit Dasgupta is supported by a Google Ph.D. India Fellowship. Karteek Alahari is supported
					in part by the ANR grant AVENUE (ANR-18-CE23-0011).
					<br>
					<br>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

